{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0173ac91",
      "metadata": {
        "id": "0173ac91"
      },
      "source": [
        "\n",
        "# ITAI 2373 Module 05: Part-of-Speech Tagging\n",
        "## In-Class Exercise & Homework Lab\n",
        "\n",
        "Welcome to the world of Part-of-Speech (POS) tagging - the \"grammar police\" of Natural Language Processing! üöîüìù\n",
        "\n",
        "In this notebook, you'll explore how computers understand the grammatical roles of words in sentences, from simple rule-based approaches to modern AI systems.\n",
        "\n",
        "### What You'll Learn:\n",
        "- **Understand POS tagging fundamentals** and why it matters in daily apps\n",
        "- **Use NLTK and SpaCy** for practical text analysis\n",
        "- **Navigate different tag sets** and understand their trade-offs\n",
        "- **Handle real-world messy text** like speech transcripts and social media\n",
        "- **Apply POS tagging** to solve actual business problems\n",
        "\n",
        "### Structure:\n",
        "- **Part 1**: In-Class Exercise (30-45 minutes) - Basic concepts and hands-on practice\n",
        "- **Part 2**: Homework Lab - Real-world applications and advanced challenges\n",
        "\n",
        "---\n",
        "\n",
        "*üí° **Pro Tip**: POS tagging is everywhere! It helps search engines understand \"Apple stock\" vs \"apple pie\", helps Siri understand your commands, and powers autocorrect on your phone.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d96f92",
      "metadata": {
        "id": "35d96f92"
      },
      "source": [
        "\n",
        "## üõ†Ô∏è Setup and Installation\n",
        "\n",
        "Let's get our tools ready! We'll use two powerful libraries:\n",
        "- **NLTK**: The \"Swiss Army knife\" of NLP - comprehensive but requires setup\n",
        "- **SpaCy**: The \"speed demon\" - built for production, cleaner output\n",
        "\n",
        "Run the cells below to install and set up everything we need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5f81ea",
      "metadata": {
        "id": "2a5f81ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9678ca-a07c-4c05-8560-9b19bb5c91b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required libraries (run this first!)\n",
        "!pip install nltk spacy matplotlib seaborn pandas\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1135905",
      "metadata": {
        "id": "a1135905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236732e9-d5fd-4328-c07e-3ef547eb8410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ All libraries loaded successfully!\n",
            "üìö NLTK version: 3.9.1\n",
            "üöÄ SpaCy version: 3.8.7\n"
          ]
        }
      ],
      "source": [
        "# Import all the libraries we'll need\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data (this might take a moment)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"üéâ All libraries loaded successfully!\")\n",
        "print(\"üìö NLTK version:\", nltk.__version__)\n",
        "print(\"üöÄ SpaCy version:\", spacy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c816a7ce",
      "metadata": {
        "id": "c816a7ce"
      },
      "source": [
        "\n",
        "---\n",
        "# üéØ PART 1: IN-CLASS EXERCISE (30-45 minutes)\n",
        "\n",
        "Welcome to the hands-on portion! We'll start with the basics and build up your understanding step by step.\n",
        "\n",
        "## Learning Goals for Part 1:\n",
        "1. Understand what POS tagging does\n",
        "2. Use NLTK and SpaCy for basic tagging\n",
        "3. Interpret and compare different tag outputs\n",
        "4. Explore word ambiguity with real examples\n",
        "5. Compare different tagging approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76538fce",
      "metadata": {
        "id": "76538fce"
      },
      "source": [
        "\n",
        "## üîç Activity 1: Your First POS Tags (10 minutes)\n",
        "\n",
        "Let's start with the classic example: \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "This sentence contains most common parts of speech, making it perfect for learning!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d12c1b4",
      "metadata": {
        "id": "6d12c1b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f519403-7f4c-40c3-8ff0-062304b4fe99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The quick brown fox jumps over the lazy dog\n",
            "\n",
            "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "\n",
            "POS Tags:\n",
            "  The      -> DT\n",
            "  quick    -> JJ\n",
            "  brown    -> NN\n",
            "  fox      -> NN\n",
            "  jumps    -> VBZ\n",
            "  over     -> IN\n",
            "  the      -> DT\n",
            "  lazy     -> JJ\n",
            "  dog      -> NN\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Let's start with a classic example\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# TODO: Use NLTK to tokenize and tag the sentence\n",
        "# Hint: Use nltk.word_tokenize() and nltk.pos_tag()\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"\\nTokens:\", tokens)\n",
        "print(\"\\nPOS Tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"  {word:8} -> {tag}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555b5f5d",
      "metadata": {
        "id": "555b5f5d"
      },
      "source": [
        "\n",
        "### ü§î Quick Questions:\n",
        "1. What does 'DT' mean? What about 'JJ'?\n",
        "2. Why do you think 'brown' and 'lazy' have the same tag?\n",
        "3. Can you guess what 'VBZ' represents?\n",
        "\n",
        "*Hint: Think about the grammatical role each word plays in the sentence!*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3045611",
      "metadata": {
        "id": "f3045611"
      },
      "source": [
        "\n",
        "## üöÄ Activity 2: SpaCy vs NLTK Showdown (10 minutes)\n",
        "\n",
        "Now let's see how SpaCy handles the same sentence. SpaCy uses cleaner, more intuitive tag names.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9669b15",
      "metadata": {
        "id": "a9669b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f66643e-42e4-4827-ba29-6b3c9265ca14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy POS Tags:\n",
            "  The      -> DET    (DT)\n",
            "  quick    -> ADJ    (JJ)\n",
            "  brown    -> ADJ    (JJ)\n",
            "  fox      -> NOUN   (NN)\n",
            "  jumps    -> VERB   (VBZ)\n",
            "  over     -> ADP    (IN)\n",
            "  the      -> DET    (DT)\n",
            "  lazy     -> ADJ    (JJ)\n",
            "  dog      -> NOUN   (NN)\n",
            "\n",
            "==================================================\n",
            "COMPARISON:\n",
            "==================================================\n",
            "Word       NLTK     SpaCy     \n",
            "------------------------------\n",
            "The        DT       DET       \n",
            "quick      JJ       ADJ       \n",
            "brown      NN       ADJ       \n",
            "fox        NN       NOUN      \n",
            "jumps      VBZ      VERB      \n",
            "over       IN       ADP       \n",
            "the        DT       DET       \n",
            "lazy       JJ       ADJ       \n",
            "dog        NN       NOUN      \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: Process the same sentence with SpaCy\n",
        "# Hint: Use nlp(sentence) and access .text and .pos_ attributes\n",
        "\n",
        "doc = nlp(sentence)\n",
        "\n",
        "\n",
        "print(\"SpaCy POS Tags:\")\n",
        "for token in doc:\n",
        "    print(f\"  {token.text:8} -> {token.pos_:6} ({token.tag_})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"COMPARISON:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Let's compare side by side\n",
        "nltk_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "spacy_doc = nlp(sentence)\n",
        "\n",
        "print(f\"{'Word':10} {'NLTK':8} {'SpaCy':10}\")\n",
        "print(\"-\" * 30)\n",
        "for i, (word, nltk_tag) in enumerate(nltk_tags):\n",
        "    spacy_tag = spacy_doc[i].pos_\n",
        "    print(f\"{word:10} {nltk_tag:8} {spacy_tag:10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889c2fcc",
      "metadata": {
        "id": "889c2fcc"
      },
      "source": [
        "\n",
        "### üéØ Discussion Points:\n",
        "- Which tags are easier to understand: NLTK's or SpaCy's?\n",
        "- Do you notice any differences in how they tag the same words?\n",
        "- Which system would you prefer for a beginner? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d773576",
      "metadata": {
        "id": "1d773576"
      },
      "source": [
        "\n",
        "## üé≠ Activity 3: The Ambiguity Challenge (15 minutes)\n",
        "\n",
        "Here's where things get interesting! Many words can be different parts of speech depending on context. Let's explore this with some tricky examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de9076f",
      "metadata": {
        "id": "4de9076f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6862e64d-5d2b-4da9-d3c5-ce4ffc4f42b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé≠ AMBIGUITY EXPLORATION\n",
            "========================================\n",
            "\n",
            "Sentence: I will lead the team to victory.\n",
            "  üéØ 'lead' is tagged as: VB\n",
            "\n",
            "Sentence: The lead pipe is heavy.\n",
            "  üéØ 'lead' is tagged as: NN\n",
            "\n",
            "Sentence: She took the lead in the race.\n",
            "  üéØ 'lead' is tagged as: NN\n",
            "\n",
            "Sentence: The bank approved my loan.\n",
            "  üéØ 'bank' is tagged as: NN\n",
            "\n",
            "Sentence: We sat by the river bank.\n",
            "  üéØ 'bank' is tagged as: NN\n",
            "\n",
            "Sentence: I bank with Chase.\n",
            "  üéØ 'bank' is tagged as: NN\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ambiguous words in different contexts\n",
        "ambiguous_sentences = [\n",
        "    \"I will lead the team to victory.\",           # lead = verb\n",
        "    \"The lead pipe is heavy.\",                    # lead = noun (metal)\n",
        "    \"She took the lead in the race.\",            # lead = noun (position)\n",
        "    \"The bank approved my loan.\",                # bank = noun (financial)\n",
        "    \"We sat by the river bank.\",                 # bank = noun (shore)\n",
        "    \"I bank with Chase.\",                        # bank = verb\n",
        "]\n",
        "\n",
        "print(\"üé≠ AMBIGUITY EXPLORATION\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for sentence in ambiguous_sentences:\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "\n",
        "    # TODO: Tag each sentence and find the ambiguous word\n",
        "    # Focus on 'lead' and 'bank' - what tags do they get?\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Find and highlight the key word\n",
        "    for word, tag in tags:\n",
        "        if word.lower() in ['lead', 'bank']:\n",
        "            print(f\"  üéØ '{word}' is tagged as: {tag}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b299cc67",
      "metadata": {
        "id": "b299cc67"
      },
      "source": [
        "\n",
        "### üß† Think About It:\n",
        "1. How does the computer know the difference between \"lead\" (metal) and \"lead\" (guide)?\n",
        "2. What clues in the sentence help determine the correct part of speech?\n",
        "3. Can you think of other words that change meaning based on context?\n",
        "\n",
        "**Try This**: Add your own ambiguous sentences to the list above and see how the tagger handles them!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd61b43a",
      "metadata": {
        "id": "cd61b43a"
      },
      "source": [
        "\n",
        "## üìä Activity 4: Tag Set Showdown (10 minutes)\n",
        "\n",
        "NLTK can use different tag sets. Let's compare the detailed Penn Treebank tags (~45 tags) with the simpler Universal Dependencies tags (~17 tags).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd01009",
      "metadata": {
        "id": "9fd01009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4ed447-ad77-428a-da51-9331ec84c91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAG SET COMPARISON\n",
            "==================================================\n",
            "Word            Penn Treebank   Universal \n",
            "--------------------------------------------------\n",
            "The             DT              DET       \n",
            "brilliant       JJ              ADJ       \n",
            "students        NNS             NOUN      \n",
            "quickly         RB              ADV       \n",
            "solved          VBD             VERB      \n",
            "the             DT              DET       \n",
            "challenging     VBG             VERB      \n",
            "programming     JJ              ADJ       \n",
            "assignment      NN              NOUN      \n",
            ".               .               .         \n",
            "\n",
            "üìä Penn Treebank uses 8 different tags\n",
            "üìä Universal uses 6 different tags\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Compare different tag sets\n",
        "test_sentence = \"The brilliant students quickly solved the challenging programming assignment.\"\n",
        "\n",
        "tokens = word_tokenize(test_sentence)\n",
        "\n",
        "# TODO: Get tags using both Penn Treebank and Universal tagsets\n",
        "# Hint: Use tagset='universal' parameter for universal tags\n",
        "penn_tags = nltk.pos_tag(tokens)\n",
        "universal_tags = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "print(\"TAG SET COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Word':15} {'Penn Treebank':15} {'Universal':10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# TODO: Print comparison table\n",
        "# Hint: Zip the two tag lists together\n",
        "for (word, penn_tag), (_, univ_tag) in zip(penn_tags, universal_tags):\n",
        "    print(f\"{word:15} {penn_tag:15} {univ_tag:10}\")\n",
        "\n",
        "# Let's also visualize the tag distribution\n",
        "penn_tag_counts = Counter([tag for word, tag in penn_tags])\n",
        "univ_tag_counts = Counter([tag for word, tag in universal_tags])\n",
        "\n",
        "print(f\"\\nüìä Penn Treebank uses {len(penn_tag_counts)} different tags\")\n",
        "print(f\"üìä Universal uses {len(univ_tag_counts)} different tags\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fab1efe",
      "metadata": {
        "id": "2fab1efe"
      },
      "source": [
        "\n",
        "### ü§î Reflection Questions:\n",
        "1. Which tag set is more detailed? Which is simpler? Enter your answer below\n",
        "\n",
        "Penn Treebank is more detailed.\n",
        "Universal is simpler.\n",
        "2. When might you want detailed tags vs. simple tags? Enter your answer below\n",
        "\n",
        "Detailed tags for linguistic analysis or parsing sentences closely.\n",
        "Simple tags for broader tasks like classification or sentiment analysis.\n",
        "3. If you were building a search engine, which would you choose? Why? Enter your answer below\n",
        "\n",
        "I‚Äôd choose Universal because it‚Äôs simpler, faster, and enough to capture the main meaning of queries.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e2ce7d",
      "metadata": {
        "id": "e2e2ce7d"
      },
      "source": [
        "\n",
        "---\n",
        "# üéì End of Part 1: In-Class Exercise\n",
        "\n",
        "Great work! You've learned the fundamentals of POS tagging and gotten hands-on experience with both NLTK and SpaCy.\n",
        "\n",
        "## What You've Accomplished:\n",
        "‚úÖ Used NLTK and SpaCy for basic POS tagging  \n",
        "‚úÖ Interpreted different tag systems  \n",
        "‚úÖ Explored word ambiguity and context  \n",
        "‚úÖ Compared different tagging approaches  \n",
        "\n",
        "## üè† Ready for Part 2?\n",
        "The homework lab will challenge you with real-world applications, messy data, and advanced techniques. You'll analyze customer service transcripts, handle informal language, and benchmark different taggers.\n",
        "\n",
        "**Take a break, then dive into Part 2 when you're ready!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571e9ac8",
      "metadata": {
        "id": "571e9ac8"
      },
      "source": [
        "\n",
        "# üè† PART 2: HOMEWORK LAB\n",
        "## Real-World POS Tagging Challenges\n",
        "\n",
        "Welcome to the advanced section! Here you'll tackle the messy, complex world of real text data. This is where POS tagging gets interesting (and challenging)!\n",
        "\n",
        "## Learning Goals for Part 2:\n",
        "1. Process real-world, messy text data\n",
        "2. Handle speech transcripts and informal language\n",
        "3. Analyze customer service scenarios\n",
        "4. Benchmark and compare different taggers\n",
        "5. Understand limitations and edge cases\n",
        "\n",
        "## üìã Submission Requirements:\n",
        "- Complete all exercises with working code\n",
        "- Answer all reflection questions\n",
        "- Include at least one visualization\n",
        "- Submit your completed notebook file\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ae7ff1",
      "metadata": {
        "id": "15ae7ff1"
      },
      "source": [
        "\n",
        "## üåç Lab Exercise 1: Messy Text Challenge (25 minutes)\n",
        "\n",
        "Real-world text is nothing like textbook examples! Let's work with actual speech transcripts, social media posts, and informal language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacc5fe7",
      "metadata": {
        "id": "cacc5fe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e26ef8-461c-4b74-e5ff-19c446e5bafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç PROCESSING MESSY TEXT\n",
            "============================================================\n",
            "\n",
            "üìù Sample 1: Um, so like, I was gonna say that, uh, the system ain't working right, you know?\n",
            "----------------------------------------\n",
            "NLTK problematic words: ['Um', 'like', 'uh']\n",
            "SpaCy problematic words: ['Um', 'like', 'uh', \"n't\"]\n",
            "NLTK success rate: 83.3%\n",
            "SpaCy success rate: 77.8%\n",
            "\n",
            "üìù Sample 2: OMG this app is sooo buggy rn üò§ cant even login smh\n",
            "----------------------------------------\n",
            "NLTK problematic words: ['OMG', 'rn', 'smh']\n",
            "SpaCy problematic words: ['OMG', 'rn', 'üò§', 'smh']\n",
            "NLTK success rate: 75.0%\n",
            "SpaCy success rate: 69.2%\n",
            "\n",
            "üìù Sample 3: Yeah hi um I'm calling because my internet's been down since like yesterday and I've tried unplugging the router thingy but it's still not working\n",
            "----------------------------------------\n",
            "NLTK problematic words: ['um', 'like']\n",
            "SpaCy problematic words: ['um', \"'m\", \"'s\", 'like', \"'ve\", \"'s\"]\n",
            "NLTK success rate: 93.1%\n",
            "SpaCy success rate: 79.3%\n",
            "\n",
            "üìù Sample 4: Y'all better fix this ASAP cuz I'm bout to switch providers fr fr\n",
            "----------------------------------------\n",
            "NLTK problematic words: [\"Y'all\", 'cuz', 'bout', 'fr', 'fr']\n",
            "SpaCy problematic words: [\"Y'all\", 'cuz', \"'m\", 'bout', 'fr', 'fr']\n",
            "NLTK success rate: 64.3%\n",
            "SpaCy success rate: 57.1%\n",
            "\n",
            "üìù Sample 5: The API endpoint is returning a 500 error but idk why it's happening tbh\n",
            "----------------------------------------\n",
            "NLTK problematic words: ['500', 'idk', 'tbh']\n",
            "SpaCy problematic words: ['500', 'idk', \"'s\", 'tbh']\n",
            "NLTK success rate: 80.0%\n",
            "SpaCy success rate: 73.3%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Real-world messy text samples\n",
        "messy_texts = [\n",
        "    # Speech transcript with disfluencies\n",
        "    \"Um, so like, I was gonna say that, uh, the system ain't working right, you know?\",\n",
        "\n",
        "    # Social media style\n",
        "    \"OMG this app is sooo buggy rn üò§ cant even login smh\",\n",
        "\n",
        "    # Customer service transcript\n",
        "    \"Yeah hi um I'm calling because my internet's been down since like yesterday and I've tried unplugging the router thingy but it's still not working\",\n",
        "\n",
        "    # Informal contractions and slang\n",
        "    \"Y'all better fix this ASAP cuz I'm bout to switch providers fr fr\",\n",
        "\n",
        "    # Technical jargon mixed with casual speech\n",
        "    \"The API endpoint is returning a 500 error but idk why it's happening tbh\"\n",
        "]\n",
        "\n",
        "print(\"üîç PROCESSING MESSY TEXT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# TODO: Process each messy text sample\n",
        "# 1. Use both NLTK and SpaCy\n",
        "# 2. Count how many words each tagger fails to recognize properly\n",
        "# 3. Identify problematic words (slang, contractions, etc.)\n",
        "\n",
        "SLANG = {\n",
        "    \"omg\",\"rn\",\"smh\",\"y'all\",\"ya'll\",\"aint\",\"ain't\",\"cuz\",\"bout\",\"fr\",\"tbh\",\"idk\",\"ya\",\n",
        "    \"uh\",\"um\",\"like\",\"gonna\",\"wanna\",\"gotta\"\n",
        "}\n",
        "\n",
        "def non_trivial_tokens_nltk(tokens):\n",
        "    # count tokens that are words/emojis/hashtags/mentions/numbers (skip pure punctuation)\n",
        "    return [t for t in tokens if any(ch.isalnum() for ch in t) or t in {\"ü§î\",\"üò§\"} or t.startswith((\"#\",\"@\"))]\n",
        "\n",
        "\n",
        "for i, text in enumerate(messy_texts, 1):\n",
        "    print(f\"\\nüìù Sample {i}: {text}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # NLTK processing\n",
        "    nltk_tokens = nltk.word_tokenize(text)\n",
        "    nltk_tags = nltk.pos_tag(nltk_tokens)\n",
        "\n",
        "    # TODO: SpaCy processing\n",
        "    spacy_doc = nlp(text)\n",
        "\n",
        "    # TODO: Find problematic words (tagged as 'X' or unknown)\n",
        "\n",
        "\n",
        "    problematic_nltk = []\n",
        "    for w, tag in nltk_tags:\n",
        "        wl = w.lower()\n",
        "        if (wl in SLANG) or (tag == \"FW\"):\n",
        "            problematic_nltk.append(w)\n",
        "        elif not (w.isalpha() or all(ch.isalpha() or ch in {\"'\", \"-\"} for ch in w)):\n",
        "            # capture emojis, urls-ish, etc., but don't double-count pure punctuation\n",
        "            if any(ch.isalnum() for ch in w) or not w.isprintable():\n",
        "                problematic_nltk.append(w)\n",
        "\n",
        "    problematic_spacy = [\n",
        "        t.text for t in spacy_doc\n",
        "        if (t.pos_ == \"X\") or t.is_currency or t.is_space is False and (not t.is_alpha and not t.is_punct)\n",
        "        or t.text.lower() in SLANG\n",
        "    ]\n",
        "\n",
        "    print(f\"NLTK problematic words: {problematic_nltk}\")\n",
        "    print(f\"SpaCy problematic words: {problematic_spacy}\")\n",
        "\n",
        "    # TODO: Calculate success rate\n",
        "    nltk_considered = non_trivial_tokens_nltk(nltk_tokens)\n",
        "    spacy_considered = [t for t in spacy_doc if not t.is_punct and not t.is_space]\n",
        "\n",
        "    nltk_success_rate = 1.0 - (len(problematic_nltk) / max(1, len(nltk_considered)))\n",
        "    spacy_success_rate = 1.0 - (len(problematic_spacy) / max(1, len(spacy_considered)))\n",
        "    print(f\"NLTK success rate: {nltk_success_rate:.1%}\")\n",
        "    print(f\"SpaCy success rate: {spacy_success_rate:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a387a8",
      "metadata": {
        "id": "35a387a8"
      },
      "source": [
        "\n",
        "### üéØ Analysis Questions:\n",
        "1. Which tagger handles informal language better?\n",
        "2. What types of words cause the most problems?\n",
        "3. How might you preprocess text to improve tagging accuracy?\n",
        "4. What are the implications for real-world applications?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "966c3a77",
      "metadata": {
        "id": "966c3a77"
      },
      "source": [
        "\n",
        "## üìû Lab Exercise 2: Customer Service Analysis Case Study (30 minutes)\n",
        "\n",
        "You're working for a tech company that receives thousands of customer service calls daily. Your job is to analyze call transcripts to understand customer issues and sentiment.\n",
        "\n",
        "**Business Goal**: Automatically categorize customer problems and identify emotional language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a5ed54",
      "metadata": {
        "id": "c7a5ed54"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Simulated customer service call transcripts\n",
        "customer_transcripts = [\n",
        "    {\n",
        "        'id': 'CALL_001',\n",
        "        'transcript': \"Hi, I'm really frustrated because my account got locked and I can't access my files. I've been trying for hours and nothing works. This is completely unacceptable.\",\n",
        "        'category': 'account_access'\n",
        "    },\n",
        "    {\n",
        "        'id': 'CALL_002',\n",
        "        'transcript': \"Hello, I love your service but I'm having a small issue with the mobile app. It crashes whenever I try to upload photos. Could you please help me fix this?\",\n",
        "        'category': 'technical_issue'\n",
        "    },\n",
        "    {\n",
        "        'id': 'CALL_003',\n",
        "        'transcript': \"Your billing system charged me twice this month! I want a refund immediately. This is ridiculous and I'm considering canceling my subscription.\",\n",
        "        'category': 'billing'\n",
        "    },\n",
        "    {\n",
        "        'id': 'CALL_004',\n",
        "        'transcript': \"I'm confused about how to use the new features you added. The interface changed and I can't find anything. Can someone walk me through it?\",\n",
        "        'category': 'user_guidance'\n",
        "    }\n",
        "]\n",
        "\n",
        "# TODO: Analyze each transcript for:\n",
        "# 1. Emotional language (adjectives that indicate sentiment)\n",
        "# 2. Action words (verbs that indicate what customer wants)\n",
        "# 3. Problem indicators (nouns related to issues)\n",
        "\n",
        "analysis_results = []\n",
        "\n",
        "for call in customer_transcripts:\n",
        "    print(f\"\\nüéß Analyzing {call['id']}\")\n",
        "    print(f\"Category: {call['category']}\")\n",
        "    print(f\"Transcript: {call['transcript']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # TODO: Process with SpaCy (it's better for this task)\n",
        "    doc = nlp(call)\n",
        "\n",
        "    # TODO: Extract different types of words\n",
        "    emotional_adjectives = # YOUR CODE HERE (JJ tags, emotional words)\n",
        "    action_verbs = # YOUR CODE HERE (VB* tags)\n",
        "    problem_nouns = # YOUR CODE HERE (NN* tags related to problems)\n",
        "\n",
        "    # TODO: Calculate sentiment indicators\n",
        "    positive_words = # YOUR CODE HERE (love, great, good, etc.)\n",
        "    negative_words = # YOUR CODE HERE (frustrated, ridiculous, unacceptable, etc.)\n",
        "\n",
        "    result = {\n",
        "        'call_id': call['id'],\n",
        "        'category': call['category'],\n",
        "        'emotional_adjectives': emotional_adjectives,\n",
        "        'action_verbs': action_verbs,\n",
        "        'problem_nouns': problem_nouns,\n",
        "        'sentiment_score': len(positive_words) - len(negative_words),\n",
        "        'urgency_indicators': # TODO: Count urgent words (immediately, ASAP, etc.)\n",
        "    }\n",
        "\n",
        "    analysis_results.append(result)\n",
        "\n",
        "    print(f\"Emotional adjectives: {emotional_adjectives}\")\n",
        "    print(f\"Action verbs: {action_verbs}\")\n",
        "    print(f\"Problem nouns: {problem_nouns}\")\n",
        "    print(f\"Sentiment score: {result['sentiment_score']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db420ac",
      "metadata": {
        "id": "6db420ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "486dcb1b-7620-4f08-bc27-7f94da047e3a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'analysis_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2087202643.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convert results to DataFrame for easier analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# TODO: Create visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'analysis_results' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: Create a summary visualization\n",
        "# Hint: Use matplotlib or seaborn to create charts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Convert results to DataFrame for easier analysis\n",
        "df = pd.DataFrame(analysis_results)\n",
        "\n",
        "# TODO: Create visualizations\n",
        "# 1. Sentiment scores by category\n",
        "# 2. Most common emotional adjectives\n",
        "# 3. Action verbs frequency\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# TODO: Plot 1 - Sentiment by category\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# TODO: Plot 2 - Word frequency analysis\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# TODO: Plot 3 - Problem categorization\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# TODO: Plot 4 - Urgency analysis\n",
        "# YOUR CODE HERE\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c9a271c",
      "metadata": {
        "id": "8c9a271c"
      },
      "source": [
        "\n",
        "### üíº Business Impact Questions:\n",
        "1. How could this analysis help prioritize customer service tickets?\n",
        "2. What patterns do you notice in different problem categories?\n",
        "3. How might you automate the routing of calls based on POS analysis?\n",
        "4. What are the limitations of this approach?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22733d79",
      "metadata": {
        "id": "22733d79"
      },
      "source": [
        "\n",
        "## ‚ö° Lab Exercise 3: Tagger Performance Benchmarking (20 minutes)\n",
        "\n",
        "Let's scientifically compare different POS taggers on various types of text. This will help you understand when to use which tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ac25fc",
      "metadata": {
        "id": "39ac25fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Different text types for testing\n",
        "test_texts = {\n",
        "    'formal': \"The research methodology employed in this study follows established academic protocols.\",\n",
        "    'informal': \"lol this study is kinda weird but whatever works i guess ü§∑‚Äç‚ôÄÔ∏è\",\n",
        "    'technical': \"The API returns a JSON response with HTTP status code 200 upon successful authentication.\",\n",
        "    'conversational': \"So like, when you click that button thingy, it should totally work, right?\",\n",
        "    'mixed': \"OMG the algorithm's performance is absolutely terrible! The accuracy dropped to 23% wtf\"\n",
        "}\n",
        "\n",
        "# TODO: Benchmark different taggers\n",
        "# Test: NLTK Penn Treebank, NLTK Universal, SpaCy\n",
        "# Metrics: Speed, tag consistency, handling of unknown words\n",
        "\n",
        "benchmark_results = defaultdict(list)\n",
        "\n",
        "for text_type, text in test_texts.items():\n",
        "    print(f\"\\nüß™ Testing {text_type.upper()} text:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # TODO: NLTK Penn Treebank timing\n",
        "    start_time = time.time()\n",
        "    # YOUR CODE HERE\n",
        "    nltk_penn_time = time.time() - start_time\n",
        "\n",
        "    # TODO: NLTK Universal timing\n",
        "    start_time = time.time()\n",
        "    # YOUR CODE HERE\n",
        "    nltk_univ_time = time.time() - start_time\n",
        "\n",
        "    # TODO: SpaCy timing\n",
        "    start_time = time.time()\n",
        "    # YOUR CODE HERE\n",
        "    spacy_time = time.time() - start_time\n",
        "\n",
        "    # TODO: Count unknown/problematic tags\n",
        "    nltk_unknown = # YOUR CODE HERE\n",
        "    spacy_unknown = # YOUR CODE HERE\n",
        "\n",
        "    # Store results\n",
        "    benchmark_results[text_type] = {\n",
        "        'nltk_penn_time': nltk_penn_time,\n",
        "        'nltk_univ_time': nltk_univ_time,\n",
        "        'spacy_time': spacy_time,\n",
        "        'nltk_unknown': nltk_unknown,\n",
        "        'spacy_unknown': spacy_unknown\n",
        "    }\n",
        "\n",
        "    print(f\"NLTK Penn time: {nltk_penn_time:.4f}s\")\n",
        "    print(f\"NLTK Univ time: {nltk_univ_time:.4f}s\")\n",
        "    print(f\"SpaCy time: {spacy_time:.4f}s\")\n",
        "    print(f\"NLTK unknown words: {nltk_unknown}\")\n",
        "    print(f\"SpaCy unknown words: {spacy_unknown}\")\n",
        "\n",
        "# TODO: Create performance comparison visualization\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a656d62d",
      "metadata": {
        "id": "a656d62d"
      },
      "source": [
        "\n",
        "### üìä Performance Analysis:\n",
        "1. Which tagger is fastest? Does speed matter for your use case?\n",
        "2. Which handles informal text best?\n",
        "3. How do the taggers compare on technical jargon?\n",
        "4. What trade-offs do you see between speed and accuracy?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08244956",
      "metadata": {
        "id": "08244956"
      },
      "source": [
        "\n",
        "## üö® Lab Exercise 4: Edge Cases and Error Analysis (15 minutes)\n",
        "\n",
        "Every system has limitations. Let's explore the edge cases where POS taggers struggle and understand why.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e4119e",
      "metadata": {
        "id": "b5e4119e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Challenging edge cases\n",
        "edge_cases = [\n",
        "    \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\",  # Famous ambiguous sentence\n",
        "    \"Time flies like an arrow; fruit flies like a banana.\",              # Classic ambiguity\n",
        "    \"The man the boat the river.\",                                       # Garden path sentence\n",
        "    \"Police police Police police police police Police police.\",          # Recursive structure\n",
        "    \"James while John had had had had had had had had had had had a better effect on the teacher.\",  # Had had had...\n",
        "    \"Can can can can can can can can can can.\",                         # Modal/noun ambiguity\n",
        "    \"@username #hashtag http://bit.ly/abc123 üòÇüî•üíØ\",                   # Social media elements\n",
        "    \"COVID-19 AI/ML IoT APIs RESTful microservices\",                    # Modern technical terms\n",
        "]\n",
        "\n",
        "print(\"üö® EDGE CASE ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# TODO: Process each edge case and analyze failures\n",
        "for i, text in enumerate(edge_cases, 1):\n",
        "    print(f\"\\nüîç Edge Case {i}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    try:\n",
        "        # TODO: Process with both taggers\n",
        "        nltk_tags = # YOUR CODE HERE\n",
        "        spacy_doc = # YOUR CODE HERE\n",
        "\n",
        "        # TODO: Identify potential errors or weird tags\n",
        "        # Look for: repeated tags, unusual patterns, X tags, etc.\n",
        "\n",
        "        print(\"NLTK tags:\", [(w, t) for w, t in nltk_tags])\n",
        "        print(\"SpaCy tags:\", [(token.text, token.pos_) for token in spacy_doc])\n",
        "\n",
        "        # TODO: Analyze what went wrong\n",
        "        # YOUR ANALYSIS CODE HERE\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing: {e}\")\n",
        "\n",
        "# TODO: Reflection on limitations\n",
        "print(\"\\nü§î REFLECTION ON LIMITATIONS:\")\n",
        "print(\"=\" * 40)\n",
        "# YOUR REFLECTION CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969fe260",
      "metadata": {
        "id": "969fe260"
      },
      "source": [
        "\n",
        "### üß† Critical Thinking Questions:\n",
        "Enter you asnwers below each question.\n",
        "1. Why do these edge cases break the taggers?\n",
        "\n",
        "2. How might you preprocess text to handle some of these issues?\n",
        "\n",
        "3. When would these limitations matter in real applications?\n",
        "\n",
        "4. How do modern large language models handle these cases differently?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa06861",
      "metadata": {
        "id": "4aa06861"
      },
      "source": [
        "\n",
        "## üéØ Final Reflection and Submission\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of POS tagging, from basic concepts to real-world challenges.\n",
        "\n",
        "### üìù Reflection Questions (Answer in the cell below):\n",
        "\n",
        "1. **Tool Comparison**: Based on your experience, when would you choose NLTK vs SpaCy? Consider factors like ease of use, accuracy, speed, and application type.\n",
        "\n",
        "2. **Real-World Applications**: Describe a specific business problem where POS tagging would be valuable. How would you implement it?\n",
        "\n",
        "3. **Limitations and Solutions**: What are the biggest limitations you discovered? How might you work around them?\n",
        "\n",
        "4. **Future Learning**: What aspects of POS tagging would you like to explore further? (Neural approaches, custom training, domain adaptation, etc.)\n",
        "\n",
        "5. **Integration**: How does POS tagging fit into larger NLP pipelines? What other NLP tasks might benefit from POS information?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c5480f",
      "metadata": {
        "id": "b1c5480f"
      },
      "source": [
        "\n",
        "### ‚úçÔ∏è Your Reflection (Write your answers here):\n",
        "**Remember Reflection is not description!**\n",
        "\n",
        "**1. Tool Comparison:**\n",
        "[Your answer here]\n",
        "\n",
        "**2. Real-World Applications:**\n",
        "[Your answer here]\n",
        "\n",
        "**3. Limitations and Solutions:**\n",
        "[Your answer here]\n",
        "\n",
        "**4. Future Learning:**\n",
        "[Your answer here]\n",
        "\n",
        "**5. Integration:**\n",
        "[Your answer here]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96f81e5",
      "metadata": {
        "id": "e96f81e5"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## üì§ Submission Checklist\n",
        "\n",
        "Before submitting your completed notebook, make sure you have:\n",
        "\n",
        "- [ ] ‚úÖ Completed all TODO sections with working code\n",
        "- [ ] ‚úÖ Answered all reflection questions thoughtfully\n",
        "- [ ] ‚úÖ Created at least one meaningful visualization\n",
        "- [ ] ‚úÖ Tested your code and fixed any errors\n",
        "- [ ] ‚úÖ Added comments explaining your approach\n",
        "- [ ] ‚úÖ Included insights from your analysis\n",
        "\n",
        "### üìã Submission Instructions:\n",
        "1. **Save your notebook**: File ‚Üí Save (or Ctrl+S)\n",
        "2. **Download**: File ‚Üí Download ‚Üí Download .ipynb\n",
        "3. **Submit**: Upload your completed notebook file to the course management system\n",
        "4. **Filename**: Use format: `L05_LastName_FirstName_ITAI2373.ipynb or pdf`  \n",
        "\n",
        "### üèÜ Grading Criteria:\n",
        "- **Code Completion (40%)**: All exercises completed with working code\n",
        "- **Analysis Quality (30%)**: Thoughtful interpretation of results\n",
        "- **Reflection Depth (20%)**: Insightful answers to reflection questions  \n",
        "- **Code Quality (10%)**: Clean, commented, well-organized code\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Great Work!\n",
        "\n",
        "You've successfully explored the fascinating world of POS tagging! You now understand how computers parse human language and can apply these techniques to solve real-world problems.\n",
        "\n",
        "\n",
        "Keep exploring and happy coding! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}